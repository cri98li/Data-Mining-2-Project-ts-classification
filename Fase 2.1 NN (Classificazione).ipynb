{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import ast\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from scikitplot.metrics import plot_precision_recall\n",
    "from scikitplot.metrics import plot_roc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.rcParams['axes.formatter.useoffset']=False\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3000x1500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('flat_df_no_nulli.csv', index_col=0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = dict()\n",
    "column2encode = ['ALBUM_type_']\n",
    "\n",
    "for col in column2encode:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "df = df[df.TRACK_genre_top.isna() == False] #droppo i nulli\n",
    "\n",
    "label_encoders = dict()\n",
    "column2encode = ['TRACK_genre_top']\n",
    "\n",
    "for col in column2encode:\n",
    "    le = LabelEncoder()\n",
    "    df['TRACK_genre_top_num'] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "df['TRACK_genre_top_num']=[0 if genre == 3 else genre for genre in df['TRACK_genre_top_num']]\n",
    "df['TRACK_genre_top_num']=[0 if genre == 0 else genre for genre in df['TRACK_genre_top_num']]\n",
    "df['TRACK_genre_top_num']=[0 if genre == 14 else genre for genre in df['TRACK_genre_top_num']]\n",
    "df['TRACK_genre_top_num']=[0 if genre == 2 else genre for genre in df['TRACK_genre_top_num']]\n",
    "\n",
    "\n",
    "\n",
    "df['TRACK_genre_top_num']=[0 if genre == 15 else genre for genre in df['TRACK_genre_top_num']]\n",
    "df['TRACK_genre_top_num']=[0 if genre == 11 else genre for genre in df['TRACK_genre_top_num']]\n",
    "df['TRACK_genre_top_num']=[0 if genre == 10 else genre for genre in df['TRACK_genre_top_num']]\n",
    "\n",
    "\n",
    "df['TRACK_genre_top_num']=[3 if genre == 13 else genre for genre in df['TRACK_genre_top_num']] #rock 13 --> 3\n",
    "df['TRACK_genre_top_num']=[2 if genre == 12 else genre for genre in df['TRACK_genre_top_num']] #pop 12 --> 2\n",
    "\n",
    "df.TRACK_genre_top_num.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALBUM_date_released_year_',\n",
       " 'ALBUM_favorites_',\n",
       " 'ALBUM_listens_',\n",
       " 'ALBUM_tracks_',\n",
       " 'ALBUM_type_',\n",
       " 'ARTIST_favorites_',\n",
       " 'TRACK_bitrate',\n",
       " 'TRACK_duration',\n",
       " 'TRACK_genre_top_num',\n",
       " 'TRACK_interest',\n",
       " 'TRACK_listens',\n",
       " 'TRACK_number']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    " 'chroma_censkurtosis',\n",
    " 'chroma_censmax',\n",
    " 'chroma_censmean',\n",
    " 'chroma_censmedian',\n",
    " 'chroma_censmin',\n",
    " 'chroma_censskew',\n",
    " 'chroma_censstd',\n",
    " 'chroma_censcqtkurtosis',\n",
    " 'chroma_cqtmax',\n",
    " 'chroma_cqtmean',\n",
    " 'chroma_cqtmedian',\n",
    " 'chroma_cqtmin',\n",
    " 'chroma_cqtskew',\n",
    " 'chroma_cqtstd',\n",
    " 'chroma_stftkurtosis',\n",
    " 'chroma_stftmax',\n",
    " 'chroma_stftmean',\n",
    " 'chroma_stftmedian',\n",
    " 'chroma_stftmin',\n",
    " 'chroma_stftskew',\n",
    " 'chroma_stftstd',\n",
    " 'mfcckurtosis',\n",
    " 'mfccmax',\n",
    " 'mfccmean',\n",
    " 'mfccmedian',\n",
    " 'mfccmin',\n",
    " 'mfccskew',\n",
    " 'mfccstd',\n",
    " 'spectral_contrastkurtosis',\n",
    " 'spectral_contrastmax',\n",
    " 'spectral_contrastmean',\n",
    " 'spectral_contrastmedian',\n",
    " 'spectral_contrastmin',\n",
    " 'spectral_contrastskew',\n",
    " 'spectral_contraststd',\n",
    " 'tonnetzkurtosis',\n",
    " 'tonnetzmax',\n",
    " 'tonnetzmean',\n",
    " 'tonnetzmedian',\n",
    " 'tonnetzmin',\n",
    " 'tonnetzskew',\n",
    " 'tonnetzstd',\n",
    " 'rmsekurtosis01',\n",
    " 'rmsemax01',\n",
    " 'rmsemean01',\n",
    " 'rmsemedian01',\n",
    " 'rmsemin01',\n",
    " 'rmseskew01',\n",
    " 'rmsestd01',\n",
    " 'spectral_bandwidthkurtosis01',\n",
    " 'spectral_bandwidthmax01',\n",
    " 'spectral_bandwidthmean01',\n",
    " 'spectral_bandwidthmedian01',\n",
    " 'spectral_bandwidthmin01',\n",
    " 'spectral_bandwidthskew01',\n",
    " 'spectral_bandwidthstd01',\n",
    " 'spectral_centroidkurtosis01',\n",
    " 'spectral_centroidmax01',\n",
    " 'spectral_centroidmean01',\n",
    " 'spectral_centroidmedian01',\n",
    " 'spectral_centroidmin01',\n",
    " 'spectral_centroidskew01',\n",
    " 'spectral_centroidstd01',\n",
    " 'spectral_rolloffkurtosis01',\n",
    " 'spectral_rolloffmax01',\n",
    " 'spectral_rolloffmean01',\n",
    " 'spectral_rolloffmedian01',\n",
    " 'spectral_rolloffmin01',\n",
    " 'spectral_rolloffskew01',\n",
    " 'spectral_rolloffstd01',\n",
    " 'zcrkurtosis01',\n",
    " 'zcrmax01',\n",
    " 'zcrmean01',\n",
    " 'zcrmedian01',\n",
    " 'zcrmin01',\n",
    " 'zcrskew01',\n",
    " 'zcrstd01',\n",
    "]\n",
    "\n",
    "\n",
    "bitmap = [\n",
    "    'Blues',\n",
    " 'Classical',\n",
    " 'Country',\n",
    " 'Easy Listening',\n",
    " 'Electronic',\n",
    " 'Experimental',\n",
    " 'Folk',\n",
    " 'Hip-Hop',\n",
    " 'Instrumental',\n",
    " 'International',\n",
    " 'Jazz',\n",
    " 'Old-Time / Historic',\n",
    " 'Pop',\n",
    " 'Rock',\n",
    " 'Soul-RnB',\n",
    " 'Spoken',\n",
    "]\n",
    "\n",
    "nulli = [\n",
    "     'ALBUM_date_released_year',\n",
    "    'ALBUM_favorites',\n",
    "     'ALBUM_listens',\n",
    " 'ALBUM_tracks',\n",
    " 'ALBUM_type',\n",
    " 'ARTIST_favorites',\n",
    " 'ARTIST_location',\n",
    " 'TRACK_favorites',\n",
    "  'ARTIST_location_',\n",
    "    'TRACK_genres_soloTop'\n",
    "]\n",
    "\n",
    "nonNumerici = [\n",
    "    'ALBUM_tags',\n",
    "    'ALBUM_title',\n",
    "    'ARTIST_location_',\n",
    "    'ARTIST_name',\n",
    "    'ARTIST_tags',\n",
    "    'SET_subset',\n",
    "    'TRACK_genre_top',\n",
    "    'TRACK_genres',\n",
    "    'TRACK_genres_all',\n",
    "    'TRACK_genres_soloTop',\n",
    "    'TRACK_tags',\n",
    "    'TRACK_title',\n",
    "    'TRACK_genres_soloTop'\n",
    "]\n",
    "\n",
    "def Diff(li1, li2):\n",
    "    return list(set(li1)-set(li2))\n",
    "\n",
    "tutti = list(df.columns)\n",
    "\n",
    "compl = Diff(tutti, features + bitmap + nonNumerici + nulli+ ['target'])\n",
    "\n",
    "compl.sort()\n",
    "\n",
    "compl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OPZ] codice per creare le bitmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitmaps = {}\n",
    "bitmaps[0] = pd.get_dummies(df[\"ALBUM_date_released_year_\"].apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "bitmaps[1] = pd.get_dummies(df[\"ALBUM_tracks_\"].apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "bitmaps[2] = pd.get_dummies(df[\"ALBUM_type_\"].apply(pd.Series).stack()).sum(level=0)\n",
    "bitmaps[3] = pd.get_dummies(df[\"TRACK_bitrate\"].apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    df = pd.concat([df, bitmaps[i]], axis=1)\n",
    "\n",
    "    \n",
    "df.drop(columns=[\"ALBUM_date_released_year_\", 'ALBUM_tracks_', 'ALBUM_type_', 'TRACK_bitrate'], inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ricalcolo compl\n",
    "def Diff(li1, li2):\n",
    "    return list(set(li1)-set(li2))\n",
    "\n",
    "tutti = list(df.columns)\n",
    "\n",
    "compl = Diff(tutti, features + bitmap + nonNumerici + nulli+ ['target'])\n",
    "\n",
    "compl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OPZ] codice per reinserire tutte le features\n",
    "\n",
    "### ATTENZIONE: non rimuove le features ottenute con la PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresdf = pd.read_csv('fma_metadata_progetto/features.csv', index_col=0, header=[0, 1, 2])\n",
    "\n",
    "flat_features = pd.DataFrame()\n",
    "\n",
    "for l1 in featuresdf.columns:\n",
    "    name = \"\"\n",
    "    for l2 in l1:\n",
    "        name = name + l2\n",
    "        \n",
    "    flat_features[name] = featuresdf[l1]\n",
    "    \n",
    "df = pd.merge(df, flat_features, left_index=True, right_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ricalcolo compl\n",
    "def Diff(li1, li2):\n",
    "    return list(set(li1)-set(li2))\n",
    "\n",
    "tutti = list(df.columns)\n",
    "\n",
    "compl = Diff(tutti, features + bitmap + nonNumerici + nulli+ ['target'])\n",
    "\n",
    "\n",
    "compl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "hidden_layer_sizes tuple, length = n_layers - 2, default=(100,)\n",
    "The ith element represents the number of neurons in the ith hidden layer.\n",
    "\n",
    "activation {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
    "Activation function for the hidden layer.\n",
    "* 'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "* 'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "* 'tanh', the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "* 'relu', the rectified linear unit function, returns f(x) = max(0, x)\n",
    "\n",
    "solver {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
    "The solver for weight optimization.\n",
    "* 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
    "* 'sgd' refers to stochastic gradient descent.\n",
    "* 'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "The default solver 'adam' works pretty well on relatively large datasets (>= 1000 training samples) in terms of both training time and validation score. For small datasets, 'lbfgs' can converge faster and perform better.\n",
    "\n",
    "alpha float, default=0.0001\n",
    "L2 penalty (regularization term) parameter.\n",
    "\n",
    "batch_size int, default='auto'\n",
    "Size of minibatches for stochastic optimizers. If the solver is 'lbfgs', the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)\n",
    "\n",
    "learning_rate {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
    "Learning rate schedule for weight updates.\n",
    "*'constant' is a constant learning rate given by 'learning_rate_init'.\n",
    "*'invscaling' gradually decreases the learning rate at each time step 't' using an inverse scaling exponent of *'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "*'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5.\n",
    "Only used when solver='sgd'.\n",
    "\n",
    "learning_rate_init double, default=0.001\n",
    "The initial learning rate used. It controls the step-size in updating the weights. Only used when solver='sgd' or 'adam'.\n",
    "\n",
    "power_t double, default=0.5\n",
    "The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'.\n",
    "\n",
    "max_iter int, default=200\n",
    "Maximum number of iterations. The solver iterates until convergence (determined by 'tol') or this number of iterations. For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "\n",
    "shuffle bool, default=True\n",
    "Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'.\n",
    "\n",
    "random_state int, RandomState instance or None, default=None\n",
    "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "tol float, default=1e-4\n",
    "Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to 'adaptive', convergence is considered to be reached and training stops.\n",
    "\n",
    "verbose bool, default=False\n",
    "Whether to print progress messages to stdout.\n",
    "\n",
    "warm_start bool, default=False\n",
    "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary.\n",
    "\n",
    "momentum float, default=0.9\n",
    "Momentum for gradient descent update. Should be between 0 and 1. Only used when solver='sgd'.\n",
    "\n",
    "early_stopping bool, default=False\n",
    "Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs. The split is stratified, except in a multilabel setting. Only effective when solver='sgd' or 'adam'\n",
    "\n",
    "validation_fraction float, default=0.1\n",
    "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
    "\n",
    "beta_1 float, default=0.9\n",
    "Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver='adam'\n",
    "\n",
    "beta_2 float, default=0.999\n",
    "Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver='adam'\n",
    "\n",
    "epsilon float, default=1e-8\n",
    "Value for numerical stability in adam. Only used when solver='adam'\n",
    "\n",
    "n_iter_no_change int, default=10\n",
    "Maximum number of epochs to not meet tol improvement. Only effective when solver='sgd' or 'adam'\n",
    "\n",
    "\n",
    "### Attributes\n",
    "loss_ float\n",
    "The current loss computed with the loss function.\n",
    "\n",
    "coefs_ list, length n_layers - 1\n",
    "The ith element in the list represents the weight matrix corresponding to layer i.\n",
    "\n",
    "intercepts_ list, length n_layers - 1\n",
    "The ith element in the list represents the bias vector corresponding to layer i + 1.\n",
    "\n",
    "n_iter_ int,\n",
    "The number of iterations the solver has ran.\n",
    "\n",
    "n_layers_ int\n",
    "Number of layers.\n",
    "\n",
    "n_outputs_ int\n",
    "Number of outputs.\n",
    "\n",
    "out_activation_ string\n",
    "Name of the output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'TRACK_genre_top_num'\n",
    "\n",
    "attributes = Diff(compl + features, [class_name]) #Diff(compl + features, ('ALBUM_favorites_', 'ALBUM_listens_', 'TRACK_number', 'TRACK_interest'))\n",
    "X = df[attributes].values\n",
    "y = df[class_name]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
    "\n",
    "#Scalo i dati (ris no PCA, 1000, 500)\n",
    "scaler = StandardScaler()    #0.6931610633836163\n",
    "#scaler = MinMaxScaler()     #0.6896435094365149    ma ci mette una marea a convergere\n",
    "#scaler = RobustScaler()     #0.6920110938239871\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.20739275\n",
      "Validation score: 0.664058\n",
      "Iteration 2, loss = 0.99515856\n",
      "Validation score: 0.668696\n",
      "Iteration 3, loss = 0.91591845\n",
      "Validation score: 0.682609\n",
      "Iteration 4, loss = 0.84818798\n",
      "Validation score: 0.688116\n",
      "Iteration 5, loss = 0.78849348\n",
      "Validation score: 0.679710\n",
      "Iteration 6, loss = 0.73712344\n",
      "Validation score: 0.702319\n",
      "Iteration 7, loss = 0.68092748\n",
      "Validation score: 0.699130\n",
      "Iteration 8, loss = 0.63283353\n",
      "Validation score: 0.704638\n",
      "Iteration 9, loss = 0.57832433\n",
      "Validation score: 0.705797\n",
      "Iteration 10, loss = 0.53455138\n",
      "Validation score: 0.698841\n",
      "Iteration 11, loss = 0.47859892\n",
      "Validation score: 0.708696\n",
      "Iteration 12, loss = 0.43026993\n",
      "Validation score: 0.703768\n",
      "Iteration 13, loss = 0.38374542\n",
      "Validation score: 0.711014\n",
      "Iteration 14, loss = 0.34518499\n",
      "Validation score: 0.715362\n",
      "Iteration 15, loss = 0.28857675\n",
      "Validation score: 0.710725\n",
      "Iteration 16, loss = 0.26529384\n",
      "Validation score: 0.706957\n",
      "Iteration 17, loss = 0.22721412\n",
      "Validation score: 0.710725\n",
      "Iteration 18, loss = 0.20927743\n",
      "Validation score: 0.708696\n",
      "Iteration 19, loss = 0.16040990\n",
      "Validation score: 0.717391\n",
      "Iteration 20, loss = 0.13369923\n",
      "Validation score: 0.708406\n",
      "Iteration 21, loss = 0.13516664\n",
      "Validation score: 0.715942\n",
      "Iteration 22, loss = 0.10570007\n",
      "Validation score: 0.709565\n",
      "Iteration 23, loss = 0.08107790\n",
      "Validation score: 0.708116\n",
      "Iteration 24, loss = 0.09291885\n",
      "Validation score: 0.709275\n",
      "Iteration 25, loss = 0.07609776\n",
      "Validation score: 0.721449\n",
      "Iteration 26, loss = 0.06977344\n",
      "Validation score: 0.711884\n",
      "Iteration 27, loss = 0.04561366\n",
      "Validation score: 0.714493\n",
      "Iteration 28, loss = 0.03483040\n",
      "Validation score: 0.714203\n",
      "Iteration 29, loss = 0.02866910\n",
      "Validation score: 0.714203\n",
      "Iteration 30, loss = 0.02402848\n",
      "Validation score: 0.719130\n",
      "Iteration 31, loss = 0.13108699\n",
      "Validation score: 0.684928\n",
      "Iteration 32, loss = 0.15148716\n",
      "Validation score: 0.697681\n",
      "Iteration 33, loss = 0.08252728\n",
      "Validation score: 0.702609\n",
      "Iteration 34, loss = 0.04201069\n",
      "Validation score: 0.716232\n",
      "Iteration 35, loss = 0.02867035\n",
      "Validation score: 0.711594\n",
      "Iteration 36, loss = 0.00960822\n",
      "Validation score: 0.720290\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy 0.7084488939998647\n",
      "F1-score [0.64958829 0.83852691 0.37326813 0.79274898 0.68535936 0.72572893\n",
      " 0.62792251 0.69355632 0.55780113 0.68551237]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.58      0.65       611\n",
      "           1       0.87      0.81      0.84       364\n",
      "           2       0.43      0.33      0.37       697\n",
      "           3       0.75      0.84      0.79      4247\n",
      "           4       0.68      0.70      0.69      2778\n",
      "           5       0.74      0.72      0.73      3163\n",
      "           6       0.71      0.56      0.63       832\n",
      "           7       0.73      0.66      0.69      1061\n",
      "           8       0.56      0.56      0.56       621\n",
      "           9       0.66      0.71      0.69       409\n",
      "\n",
      "    accuracy                           0.71     14783\n",
      "   macro avg       0.68      0.65      0.66     14783\n",
      "weighted avg       0.70      0.71      0.70     14783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=0, hidden_layer_sizes=(1000, 500), learning_rate='adaptive', max_iter=1000, \n",
    "                    verbose=True, early_stopping=True)\n",
    "\n",
    "\n",
    "#top 600, 300 6964080362578637         6895758641683014 (con tutte le bitmap)\n",
    "#top 1000, 500 700872623959954         6974227152810661 <---\n",
    "#top 2000, 1000 7085841845362917       7031725630792126\n",
    "#top 4000, 2000 7109517689237638       6951227761618075\n",
    "\n",
    "#----------3\n",
    "# top 600 600 300 6950551308935939\n",
    "# top 900 600 300 7058783738077522\n",
    "# top 1200 800 400  7058783738077522\n",
    "\n",
    "#---------4 \n",
    "# top 1200 800 400 100 6990462017181898 \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train {0: 1293, 1: 751, 2: 1458, 3: 8979, 4: 5847, 5: 6632, 6: 1708, 7: 2226, 8: 1295, 9: 853}\n",
      "bilanciato {0: 1293, 1: 751, 2: 1458, 3: 8979, 4: 5847, 5: 6632, 6: 1708, 7: 2226, 8: 1295, 9: 853}\n",
      "validation {0: 132, 1: 97, 2: 170, 3: 929, 4: 634, 5: 749, 6: 233, 7: 249, 8: 154, 9: 103}\n",
      "test {0: 611, 1: 364, 2: 697, 3: 4247, 4: 2778, 5: 3163, 6: 832, 7: 1061, 8: 621, 9: 409}\n"
     ]
    }
   ],
   "source": [
    "class_name = 'TRACK_genre_top_num'\n",
    "\n",
    "attributes = Diff(compl + features, [class_name]) #Diff(compl + features, ('ALBUM_favorites_', 'ALBUM_listens_', 'TRACK_number', 'TRACK_interest'))\n",
    "X = df[attributes].values\n",
    "y = df[class_name]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "#--------------Oversampling:\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#ros = RandomOverSampler(random_state=42)\n",
    "#X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "#sm = SMOTE(random_state=42)\n",
    "#X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "X_res = X_train\n",
    "y_res = y_train\n",
    "\n",
    "#-------------Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#rus = RandomUnderSampler(random_state=42)\n",
    "#X_val, y_val = rus.fit_resample(X_val, y_val)\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler = RobustScaler()\n",
    "\n",
    "scaler.fit(X_res)\n",
    "X_res = scaler.transform(X_res)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"train\", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_res, return_counts=True)\n",
    "print(\"bilanciato\", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(\"validation\", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"test\", dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "#Trasformo la target value in vettore con 1 sulla posizione del genere giusto\n",
    "y_res = tf.keras.utils.to_categorical(y_res, 10)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI PER COSTRUIRE LA NN\n",
    "def costruisci(n_layer_crescenti=5, initialHiddenLayer=128, n_layer_decrescenti = 5, finalHiddenLayer=128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n",
    "    \n",
    "    for n in range(1, n_layer_crescenti+1):\n",
    "        model.add(Dense(initialHiddenLayer*(n), activation='relu'))\n",
    "        print(\"creo layer crescente di dimensione \", initialHiddenLayer*(n))\n",
    "    for n in range(n_layer_decrescenti , 0, -1): #CODICE DEI TEST: for n in range(n_layer_decrescenti, 0, -1):\n",
    "        model.add(Dense(finalHiddenLayer*(n), activation='relu'))\n",
    "        print(\"creo layer decrescente di dimensione \", finalHiddenLayer*(n))\n",
    "        \n",
    "    model.add(Dense(y_res.shape[1], activation=tf.nn.softmax))\n",
    "\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #<-- binario\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'],\n",
    "                  #optimizer='adam',\n",
    "                  optimizer='adamax'\n",
    "                  #optimizer='nadam'\n",
    "                  #optimizer='adadelta'\n",
    "\n",
    "                 ) #<-- multilabel/multiclass\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creo layer crescente di dimensione  800\n",
      "creo layer decrescente di dimensione  400\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.4005 - accuracy: 0.5466 - val_loss: 1.1937 - val_accuracy: 0.6058\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0889 - accuracy: 0.6382 - val_loss: 1.1149 - val_accuracy: 0.6217\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0157 - accuracy: 0.6654 - val_loss: 1.0954 - val_accuracy: 0.6310\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9374 - accuracy: 0.6899 - val_loss: 1.0470 - val_accuracy: 0.6484\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9043 - accuracy: 0.6964 - val_loss: 1.0357 - val_accuracy: 0.6559\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8442 - accuracy: 0.7181 - val_loss: 1.0315 - val_accuracy: 0.6438\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8055 - accuracy: 0.7286 - val_loss: 1.0159 - val_accuracy: 0.6617\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7609 - accuracy: 0.7429 - val_loss: 0.9975 - val_accuracy: 0.6699\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7220 - accuracy: 0.7557 - val_loss: 0.9922 - val_accuracy: 0.6739\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6687 - accuracy: 0.7736 - val_loss: 0.9942 - val_accuracy: 0.6745\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 1s 1ms/step - loss: 0.6408 - accuracy: 0.7811 - val_loss: 1.0133 - val_accuracy: 0.6794\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 1s 1ms/step - loss: 0.5992 - accuracy: 0.7974 - val_loss: 1.0156 - val_accuracy: 0.6719\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5546 - accuracy: 0.8120 - val_loss: 1.0178 - val_accuracy: 0.6809\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5226 - accuracy: 0.8267 - val_loss: 1.0370 - val_accuracy: 0.6820\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4891 - accuracy: 0.8350 - val_loss: 1.0566 - val_accuracy: 0.6852\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4489 - accuracy: 0.8513 - val_loss: 1.0635 - val_accuracy: 0.6852\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4141 - accuracy: 0.8627 - val_loss: 1.0908 - val_accuracy: 0.6826\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3802 - accuracy: 0.8773 - val_loss: 1.0858 - val_accuracy: 0.6826\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8907 - val_loss: 1.1295 - val_accuracy: 0.6797\n"
     ]
    }
   ],
   "source": [
    "model = costruisci(n_layer_crescenti=1, \n",
    "                   initialHiddenLayer=800,\n",
    "                      n_layer_decrescenti = 1,\n",
    "                      finalHiddenLayer=400)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "mc = ModelCheckpoint('best_model_NOREG.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "history1 = model.fit(X_res, y_res, epochs=1000, \n",
    "                      #batch_size= 200,\n",
    "                      #class_weight=class_weight,\n",
    "                      validation_data=(X_val, y_val), callbacks=[es,mc]\n",
    "                     ).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18183745c6c4ddaa34fd3f69041f63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db9b512b663499da049c18536badc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creo layer crescente di dimensione  800\n",
      "creo layer decrescente di dimensione  800\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 15s 14ms/step - loss: 1.3749 - accuracy: 0.5493 - val_loss: 1.1783 - val_accuracy: 0.6055\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 12s 12ms/step - loss: 1.0782 - accuracy: 0.6423 - val_loss: 1.0969 - val_accuracy: 0.6359\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 12s 12ms/step - loss: 0.9810 - accuracy: 0.6756 - val_loss: 1.0774 - val_accuracy: 0.6461\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 11s 11ms/step - loss: 0.9215 - accuracy: 0.6891 - val_loss: 1.0438 - val_accuracy: 0.6551\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 8s 8ms/step - loss: 0.8787 - accuracy: 0.7066 - val_loss: 1.0296 - val_accuracy: 0.6548\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.8142 - accuracy: 0.7273 - val_loss: 1.0164 - val_accuracy: 0.6678\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.7655 - accuracy: 0.7432 - val_loss: 1.0317 - val_accuracy: 0.6600\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.7173 - accuracy: 0.7565 - val_loss: 1.0123 - val_accuracy: 0.6707\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.6641 - accuracy: 0.7773 - val_loss: 1.0098 - val_accuracy: 0.6725\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.6236 - accuracy: 0.7905 - val_loss: 1.0243 - val_accuracy: 0.6820\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.5707 - accuracy: 0.8055 - val_loss: 1.0169 - val_accuracy: 0.6759\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.5310 - accuracy: 0.8246 - val_loss: 1.0368 - val_accuracy: 0.6806\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4888 - accuracy: 0.8372 - val_loss: 1.0736 - val_accuracy: 0.6835\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.4518 - accuracy: 0.8533 - val_loss: 1.0909 - val_accuracy: 0.6780\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4253 - accuracy: 0.8614 - val_loss: 1.0973 - val_accuracy: 0.6754\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3778 - accuracy: 0.8758 - val_loss: 1.1550 - val_accuracy: 0.6826\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3313 - accuracy: 0.8925 - val_loss: 1.1672 - val_accuracy: 0.6774\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.2840 - accuracy: 0.9140 - val_loss: 1.2092 - val_accuracy: 0.6797\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.2649 - accuracy: 0.9195 - val_loss: 1.2546 - val_accuracy: 0.6684\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.2362 - accuracy: 0.9307 - val_loss: 1.2799 - val_accuracy: 0.6803\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2026 - accuracy: 0.9414 - val_loss: 1.3166 - val_accuracy: 0.6730\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1721 - accuracy: 0.9540 - val_loss: 1.3712 - val_accuracy: 0.6707\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1499 - accuracy: 0.9622 - val_loss: 1.4043 - val_accuracy: 0.6841\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1319 - accuracy: 0.9687 - val_loss: 1.5247 - val_accuracy: 0.6757\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1147 - accuracy: 0.9723 - val_loss: 1.5243 - val_accuracy: 0.6655\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0975 - accuracy: 0.9788 - val_loss: 1.5827 - val_accuracy: 0.6765\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0787 - accuracy: 0.9849 - val_loss: 1.6024 - val_accuracy: 0.6838\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0607 - accuracy: 0.9908 - val_loss: 1.6846 - val_accuracy: 0.6751\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0543 - accuracy: 0.9925 - val_loss: 1.7414 - val_accuracy: 0.6786\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0465 - accuracy: 0.9936 - val_loss: 1.7853 - val_accuracy: 0.6780\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0367 - accuracy: 0.9963 - val_loss: 1.8797 - val_accuracy: 0.6809\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0312 - accuracy: 0.9971 - val_loss: 1.8676 - val_accuracy: 0.6780\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0278 - accuracy: 0.9974 - val_loss: 1.9388 - val_accuracy: 0.6794\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0230 - accuracy: 0.9982 - val_loss: 1.9622 - val_accuracy: 0.6797\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0171 - accuracy: 0.9993 - val_loss: 2.0447 - val_accuracy: 0.6791\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 0.9994 - val_loss: 2.1112 - val_accuracy: 0.6725\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9993 - val_loss: 2.1235 - val_accuracy: 0.6762\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0121 - accuracy: 0.9993 - val_loss: 2.2275 - val_accuracy: 0.6835\n",
      "Epoch 39/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0093 - accuracy: 0.9997 - val_loss: 2.2405 - val_accuracy: 0.6762\n",
      "108/108 [==============================] - 0s 834us/step - loss: 2.2405 - accuracy: 0.6762\n",
      "800.000000-800.000000     VAL: Loss 2.240475, Accuracy 0.676232\n",
      "creo layer crescente di dimensione  800\n",
      "creo layer decrescente di dimensione  1100\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3769 - accuracy: 0.5470 - val_loss: 1.1922 - val_accuracy: 0.6012\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0737 - accuracy: 0.6434 - val_loss: 1.1055 - val_accuracy: 0.6275\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9715 - accuracy: 0.6766 - val_loss: 1.0604 - val_accuracy: 0.6414\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8987 - accuracy: 0.6997 - val_loss: 1.0325 - val_accuracy: 0.6525\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8397 - accuracy: 0.7187 - val_loss: 1.0200 - val_accuracy: 0.6568\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7787 - accuracy: 0.7345 - val_loss: 0.9876 - val_accuracy: 0.6652\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7245 - accuracy: 0.7546 - val_loss: 0.9875 - val_accuracy: 0.6649\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6740 - accuracy: 0.7756 - val_loss: 0.9866 - val_accuracy: 0.6713\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6183 - accuracy: 0.7910 - val_loss: 1.0027 - val_accuracy: 0.6794\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5817 - accuracy: 0.8059 - val_loss: 1.0760 - val_accuracy: 0.6536\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5152 - accuracy: 0.8278 - val_loss: 1.0352 - val_accuracy: 0.6757\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4766 - accuracy: 0.8422 - val_loss: 1.0578 - val_accuracy: 0.6855\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4276 - accuracy: 0.8569 - val_loss: 1.0675 - val_accuracy: 0.6783\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3847 - accuracy: 0.8741 - val_loss: 1.1124 - val_accuracy: 0.6768\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8886 - val_loss: 1.1043 - val_accuracy: 0.6826\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.9076 - val_loss: 1.1925 - val_accuracy: 0.6757\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2617 - accuracy: 0.9208 - val_loss: 1.2472 - val_accuracy: 0.6765\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2297 - accuracy: 0.9318 - val_loss: 1.3344 - val_accuracy: 0.6614\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2034 - accuracy: 0.9407 - val_loss: 1.3173 - val_accuracy: 0.6826\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1747 - accuracy: 0.9518 - val_loss: 1.3530 - val_accuracy: 0.6786\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1436 - accuracy: 0.9628 - val_loss: 1.4311 - val_accuracy: 0.6739\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1262 - accuracy: 0.9695 - val_loss: 1.4457 - val_accuracy: 0.6809\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0995 - accuracy: 0.9778 - val_loss: 1.5201 - val_accuracy: 0.6672\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0839 - accuracy: 0.9839 - val_loss: 1.5524 - val_accuracy: 0.6713\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0688 - accuracy: 0.9898 - val_loss: 1.7024 - val_accuracy: 0.6687\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0586 - accuracy: 0.9907 - val_loss: 1.6932 - val_accuracy: 0.6704\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0484 - accuracy: 0.9931 - val_loss: 1.7728 - val_accuracy: 0.6690\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0365 - accuracy: 0.9968 - val_loss: 1.7621 - val_accuracy: 0.6710\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0298 - accuracy: 0.9978 - val_loss: 1.8573 - val_accuracy: 0.6736\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0229 - accuracy: 0.9987 - val_loss: 1.9295 - val_accuracy: 0.6809\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0221 - accuracy: 0.9983 - val_loss: 2.0012 - val_accuracy: 0.6783\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0162 - accuracy: 0.9994 - val_loss: 2.0099 - val_accuracy: 0.6771\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0142 - accuracy: 0.9995 - val_loss: 2.0877 - val_accuracy: 0.6757\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0107 - accuracy: 0.9996 - val_loss: 2.1100 - val_accuracy: 0.6716\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0110 - accuracy: 0.9992 - val_loss: 2.1783 - val_accuracy: 0.6771\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0092 - accuracy: 0.9990 - val_loss: 2.2226 - val_accuracy: 0.6800\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 2.2921 - val_accuracy: 0.6777\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0081 - accuracy: 0.9996 - val_loss: 2.3196 - val_accuracy: 0.6762\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 2.3196 - accuracy: 0.6762\n",
      "800.000000-1100.000000     VAL: Loss 2.319624, Accuracy 0.676232\n",
      "creo layer crescente di dimensione  800\n",
      "creo layer decrescente di dimensione  1400\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3834 - accuracy: 0.5484 - val_loss: 1.1615 - val_accuracy: 0.6119\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0682 - accuracy: 0.6461 - val_loss: 1.0906 - val_accuracy: 0.6348\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9752 - accuracy: 0.6745 - val_loss: 1.0481 - val_accuracy: 0.6510\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9018 - accuracy: 0.7011 - val_loss: 1.0143 - val_accuracy: 0.6565\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8325 - accuracy: 0.7201 - val_loss: 0.9991 - val_accuracy: 0.6675\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7752 - accuracy: 0.7370 - val_loss: 0.9924 - val_accuracy: 0.6777\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7274 - accuracy: 0.7538 - val_loss: 0.9856 - val_accuracy: 0.6835\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6659 - accuracy: 0.7755 - val_loss: 1.0066 - val_accuracy: 0.6832\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6011 - accuracy: 0.7978 - val_loss: 0.9926 - val_accuracy: 0.6867\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5575 - accuracy: 0.8129 - val_loss: 1.0215 - val_accuracy: 0.6783\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4983 - accuracy: 0.8330 - val_loss: 1.0309 - val_accuracy: 0.6791\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4642 - accuracy: 0.8450 - val_loss: 1.0535 - val_accuracy: 0.6777\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4106 - accuracy: 0.8671 - val_loss: 1.0850 - val_accuracy: 0.6896\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3583 - accuracy: 0.8839 - val_loss: 1.1121 - val_accuracy: 0.6791\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3137 - accuracy: 0.9034 - val_loss: 1.1173 - val_accuracy: 0.6803\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2697 - accuracy: 0.9197 - val_loss: 1.1424 - val_accuracy: 0.6916\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2287 - accuracy: 0.9335 - val_loss: 1.1986 - val_accuracy: 0.6939\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9444 - val_loss: 1.2569 - val_accuracy: 0.6843\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1677 - accuracy: 0.9561 - val_loss: 1.2969 - val_accuracy: 0.6765\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1383 - accuracy: 0.9680 - val_loss: 1.3826 - val_accuracy: 0.6719\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1163 - accuracy: 0.9730 - val_loss: 1.4289 - val_accuracy: 0.6719\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0930 - accuracy: 0.9810 - val_loss: 1.4917 - val_accuracy: 0.6725\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0755 - accuracy: 0.9874 - val_loss: 1.5306 - val_accuracy: 0.6774\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0640 - accuracy: 0.9910 - val_loss: 1.5808 - val_accuracy: 0.6786\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0515 - accuracy: 0.9926 - val_loss: 1.6605 - val_accuracy: 0.6678\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0402 - accuracy: 0.9954 - val_loss: 1.6646 - val_accuracy: 0.6788\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0327 - accuracy: 0.9971 - val_loss: 1.7635 - val_accuracy: 0.6812\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0246 - accuracy: 0.9982 - val_loss: 1.8426 - val_accuracy: 0.6823\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0244 - accuracy: 0.9977 - val_loss: 1.8630 - val_accuracy: 0.6684\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0178 - accuracy: 0.9985 - val_loss: 1.8967 - val_accuracy: 0.6765\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 3ms/step - loss: 0.0134 - accuracy: 0.9992 - val_loss: 1.9341 - val_accuracy: 0.6846\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0115 - accuracy: 0.9994 - val_loss: 2.0070 - val_accuracy: 0.6786\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971/971 [==============================] - 6s 6ms/step - loss: 0.0091 - accuracy: 0.9996 - val_loss: 2.0491 - val_accuracy: 0.6788\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0077 - accuracy: 0.9998 - val_loss: 2.1005 - val_accuracy: 0.6765\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0065 - accuracy: 0.9998 - val_loss: 2.1291 - val_accuracy: 0.6780\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 2.1686 - val_accuracy: 0.6788\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0041 - accuracy: 0.9999 - val_loss: 2.2880 - val_accuracy: 0.6814\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 2.2880 - accuracy: 0.6814\n",
      "800.000000-1400.000000     VAL: Loss 2.288001, Accuracy 0.681449\n",
      "creo layer crescente di dimensione  800\n",
      "creo layer decrescente di dimensione  1700\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 1.3966 - accuracy: 0.5415 - val_loss: 1.1737 - val_accuracy: 0.6093\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 1.0796 - accuracy: 0.6416 - val_loss: 1.1017 - val_accuracy: 0.6322\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.9839 - accuracy: 0.6741 - val_loss: 1.0559 - val_accuracy: 0.6475\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 3ms/step - loss: 0.9055 - accuracy: 0.6974 - val_loss: 1.0179 - val_accuracy: 0.6568\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8358 - accuracy: 0.7189 - val_loss: 1.0010 - val_accuracy: 0.6687\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7651 - accuracy: 0.7401 - val_loss: 1.0060 - val_accuracy: 0.6699\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7056 - accuracy: 0.7595 - val_loss: 0.9968 - val_accuracy: 0.6780\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6621 - accuracy: 0.7754 - val_loss: 0.9943 - val_accuracy: 0.6780\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5922 - accuracy: 0.7990 - val_loss: 1.0064 - val_accuracy: 0.6786\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.8218 - val_loss: 1.0189 - val_accuracy: 0.6754\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4894 - accuracy: 0.8386 - val_loss: 1.0421 - val_accuracy: 0.6794\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4283 - accuracy: 0.8617 - val_loss: 1.0799 - val_accuracy: 0.6765\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3848 - accuracy: 0.8769 - val_loss: 1.0975 - val_accuracy: 0.6841\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3538 - accuracy: 0.8888 - val_loss: 1.1308 - val_accuracy: 0.6788\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3045 - accuracy: 0.9039 - val_loss: 1.1959 - val_accuracy: 0.6838\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2534 - accuracy: 0.9243 - val_loss: 1.2044 - val_accuracy: 0.6762\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2214 - accuracy: 0.9352 - val_loss: 1.2604 - val_accuracy: 0.6803\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9505 - val_loss: 1.3025 - val_accuracy: 0.6765\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1550 - accuracy: 0.9595 - val_loss: 1.3941 - val_accuracy: 0.6780\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1250 - accuracy: 0.9713 - val_loss: 1.4550 - val_accuracy: 0.6780\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1033 - accuracy: 0.9788 - val_loss: 1.4457 - val_accuracy: 0.6765\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0819 - accuracy: 0.9832 - val_loss: 1.5284 - val_accuracy: 0.6728\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0641 - accuracy: 0.9904 - val_loss: 1.6004 - val_accuracy: 0.6814\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0540 - accuracy: 0.9933 - val_loss: 1.6493 - val_accuracy: 0.6733\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0406 - accuracy: 0.9959 - val_loss: 1.7177 - val_accuracy: 0.6742\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0340 - accuracy: 0.9965 - val_loss: 1.7436 - val_accuracy: 0.6829\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0261 - accuracy: 0.9979 - val_loss: 1.8360 - val_accuracy: 0.6814\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0194 - accuracy: 0.9990 - val_loss: 1.9752 - val_accuracy: 0.6716\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0162 - accuracy: 0.9994 - val_loss: 1.9561 - val_accuracy: 0.6768\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0133 - accuracy: 0.9994 - val_loss: 2.0651 - val_accuracy: 0.6716\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0150 - accuracy: 0.9983 - val_loss: 2.0974 - val_accuracy: 0.6890\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0104 - accuracy: 0.9995 - val_loss: 2.1047 - val_accuracy: 0.6803\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 0.0071 - accuracy: 0.9997 - val_loss: 2.1795 - val_accuracy: 0.6739\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 2.1438 - val_accuracy: 0.6745\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 2.2170 - val_accuracy: 0.6829\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0044 - accuracy: 0.9998 - val_loss: 2.2926 - val_accuracy: 0.6780\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0043 - accuracy: 0.9999 - val_loss: 2.2915 - val_accuracy: 0.6777\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 2.3657 - val_accuracy: 0.6794\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 2.3657 - accuracy: 0.6794\n",
      "800.000000-1700.000000     VAL: Loss 2.365680, Accuracy 0.679420\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c86ae77cefb46f4a45c8b5828b4dc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creo layer crescente di dimensione  1100\n",
      "creo layer decrescente di dimensione  800\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 1.3761 - accuracy: 0.5476 - val_loss: 1.1676 - val_accuracy: 0.6119\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 1.0690 - accuracy: 0.6450 - val_loss: 1.1125 - val_accuracy: 0.6307\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 0.9806 - accuracy: 0.6742 - val_loss: 1.0644 - val_accuracy: 0.6443\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.9107 - accuracy: 0.6959 - val_loss: 1.0514 - val_accuracy: 0.6417\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.8388 - accuracy: 0.7142 - val_loss: 1.0186 - val_accuracy: 0.6672\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.7739 - accuracy: 0.7386 - val_loss: 0.9892 - val_accuracy: 0.6699\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.7189 - accuracy: 0.7560 - val_loss: 0.9897 - val_accuracy: 0.6728\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.6722 - accuracy: 0.7724 - val_loss: 0.9913 - val_accuracy: 0.6667\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.6122 - accuracy: 0.7956 - val_loss: 1.0176 - val_accuracy: 0.6693\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 5s 6ms/step - loss: 0.5662 - accuracy: 0.8100 - val_loss: 1.0254 - val_accuracy: 0.6736\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.5166 - accuracy: 0.8247 - val_loss: 1.0315 - val_accuracy: 0.6814\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.4749 - accuracy: 0.8437 - val_loss: 1.0591 - val_accuracy: 0.6762\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.4263 - accuracy: 0.8608 - val_loss: 1.0748 - val_accuracy: 0.6707\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.3768 - accuracy: 0.8783 - val_loss: 1.1049 - val_accuracy: 0.6736\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.3334 - accuracy: 0.8938 - val_loss: 1.1337 - val_accuracy: 0.6800\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.2950 - accuracy: 0.9072 - val_loss: 1.2189 - val_accuracy: 0.6742\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.2641 - accuracy: 0.9212 - val_loss: 1.2386 - val_accuracy: 0.6742\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.2274 - accuracy: 0.9304 - val_loss: 1.2869 - val_accuracy: 0.6719\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.1906 - accuracy: 0.9455 - val_loss: 1.3519 - val_accuracy: 0.6777\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.1505 - accuracy: 0.9635 - val_loss: 1.4010 - val_accuracy: 0.6643\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 5s 6ms/step - loss: 0.1367 - accuracy: 0.9661 - val_loss: 1.4705 - val_accuracy: 0.6678\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.1112 - accuracy: 0.9732 - val_loss: 1.4857 - val_accuracy: 0.6817\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0943 - accuracy: 0.9790 - val_loss: 1.5628 - val_accuracy: 0.6809\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 0.0735 - accuracy: 0.9875 - val_loss: 1.5519 - val_accuracy: 0.6809\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0618 - accuracy: 0.9898 - val_loss: 1.6973 - val_accuracy: 0.6800\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0528 - accuracy: 0.9916 - val_loss: 1.7213 - val_accuracy: 0.6754\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0400 - accuracy: 0.9958 - val_loss: 1.7654 - val_accuracy: 0.6765\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0344 - accuracy: 0.9959 - val_loss: 1.8329 - val_accuracy: 0.6803\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 3ms/step - loss: 0.0280 - accuracy: 0.9973 - val_loss: 1.8942 - val_accuracy: 0.6794\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0230 - accuracy: 0.9985 - val_loss: 1.9863 - val_accuracy: 0.6780\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0186 - accuracy: 0.9986 - val_loss: 2.0030 - val_accuracy: 0.6794\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0160 - accuracy: 0.9989 - val_loss: 2.0945 - val_accuracy: 0.6739\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0131 - accuracy: 0.9996 - val_loss: 2.1148 - val_accuracy: 0.6777\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0109 - accuracy: 0.9993 - val_loss: 2.1398 - val_accuracy: 0.6791\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0105 - accuracy: 0.9994 - val_loss: 2.2009 - val_accuracy: 0.6820\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 3ms/step - loss: 0.0085 - accuracy: 0.9996 - val_loss: 2.2470 - val_accuracy: 0.6794\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.2470 - accuracy: 0.6794\n",
      "1100.000000-800.000000     VAL: Loss 2.246971, Accuracy 0.679420\n",
      "creo layer crescente di dimensione  1100\n",
      "creo layer decrescente di dimensione  1100\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 1.3919 - accuracy: 0.5380 - val_loss: 1.1972 - val_accuracy: 0.5957\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0620 - accuracy: 0.6505 - val_loss: 1.1034 - val_accuracy: 0.6272\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9832 - accuracy: 0.6733 - val_loss: 1.0603 - val_accuracy: 0.6516\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.9011 - accuracy: 0.7024 - val_loss: 1.0269 - val_accuracy: 0.6586\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8304 - accuracy: 0.7213 - val_loss: 1.0091 - val_accuracy: 0.6548\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7688 - accuracy: 0.7427 - val_loss: 1.0205 - val_accuracy: 0.6681\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.7135 - accuracy: 0.7605 - val_loss: 1.0371 - val_accuracy: 0.6681\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6509 - accuracy: 0.7796 - val_loss: 1.0059 - val_accuracy: 0.6696\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5945 - accuracy: 0.7978 - val_loss: 1.0148 - val_accuracy: 0.6780\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.5366 - accuracy: 0.8206 - val_loss: 1.0503 - val_accuracy: 0.6759\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4737 - accuracy: 0.8412 - val_loss: 1.0863 - val_accuracy: 0.6719\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8595 - val_loss: 1.0754 - val_accuracy: 0.6872\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3765 - accuracy: 0.8785 - val_loss: 1.1263 - val_accuracy: 0.6812\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3320 - accuracy: 0.8955 - val_loss: 1.1485 - val_accuracy: 0.6716\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2924 - accuracy: 0.9077 - val_loss: 1.2320 - val_accuracy: 0.6788\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 3ms/step - loss: 0.2443 - accuracy: 0.9259 - val_loss: 1.2144 - val_accuracy: 0.6791\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9421 - val_loss: 1.2927 - val_accuracy: 0.6765\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1744 - accuracy: 0.9525 - val_loss: 1.3414 - val_accuracy: 0.6725\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1392 - accuracy: 0.9650 - val_loss: 1.4202 - val_accuracy: 0.6704\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1174 - accuracy: 0.9720 - val_loss: 1.4832 - val_accuracy: 0.6701\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0961 - accuracy: 0.9791 - val_loss: 1.5719 - val_accuracy: 0.6730\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0755 - accuracy: 0.9849 - val_loss: 1.6050 - val_accuracy: 0.6742\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0589 - accuracy: 0.9911 - val_loss: 1.6548 - val_accuracy: 0.6672\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0465 - accuracy: 0.9940 - val_loss: 1.7178 - val_accuracy: 0.6806\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0367 - accuracy: 0.9963 - val_loss: 1.8190 - val_accuracy: 0.6693\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0282 - accuracy: 0.9976 - val_loss: 1.8348 - val_accuracy: 0.6794\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0250 - accuracy: 0.9978 - val_loss: 1.9259 - val_accuracy: 0.6762\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0206 - accuracy: 0.9982 - val_loss: 1.9646 - val_accuracy: 0.6762\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9992 - val_loss: 1.9864 - val_accuracy: 0.6774\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0118 - accuracy: 0.9995 - val_loss: 2.0906 - val_accuracy: 0.6710\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0100 - accuracy: 0.9994 - val_loss: 2.1386 - val_accuracy: 0.6678\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0082 - accuracy: 0.9997 - val_loss: 2.1968 - val_accuracy: 0.6655\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0079 - accuracy: 0.9997 - val_loss: 2.2134 - val_accuracy: 0.6701\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0063 - accuracy: 0.9996 - val_loss: 2.2752 - val_accuracy: 0.6733\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 2.3198 - val_accuracy: 0.6812\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0041 - accuracy: 0.9997 - val_loss: 2.3732 - val_accuracy: 0.6704\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4176 - val_accuracy: 0.6701\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 2.4323 - val_accuracy: 0.6820\n",
      "108/108 [==============================] - 0s 833us/step - loss: 2.4323 - accuracy: 0.6820\n",
      "1100.000000-1100.000000     VAL: Loss 2.432262, Accuracy 0.682029\n",
      "creo layer crescente di dimensione  1100\n",
      "creo layer decrescente di dimensione  1400\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3801 - accuracy: 0.5519 - val_loss: 1.1521 - val_accuracy: 0.6096\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0492 - accuracy: 0.6521 - val_loss: 1.1000 - val_accuracy: 0.6383\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9699 - accuracy: 0.6789 - val_loss: 1.0444 - val_accuracy: 0.6501\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8991 - accuracy: 0.6983 - val_loss: 1.0107 - val_accuracy: 0.6583\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8252 - accuracy: 0.7215 - val_loss: 0.9895 - val_accuracy: 0.6704\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7621 - accuracy: 0.7459 - val_loss: 0.9784 - val_accuracy: 0.6733\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7078 - accuracy: 0.7609 - val_loss: 1.0007 - val_accuracy: 0.6609\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6328 - accuracy: 0.7859 - val_loss: 1.0019 - val_accuracy: 0.6794\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5915 - accuracy: 0.7980 - val_loss: 1.0077 - val_accuracy: 0.6901\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5299 - accuracy: 0.8230 - val_loss: 1.0328 - val_accuracy: 0.6707\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4665 - accuracy: 0.8424 - val_loss: 1.0500 - val_accuracy: 0.6835\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8582 - val_loss: 1.0734 - val_accuracy: 0.6823\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3600 - accuracy: 0.8823 - val_loss: 1.1146 - val_accuracy: 0.6838\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3161 - accuracy: 0.8973 - val_loss: 1.1589 - val_accuracy: 0.6812\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2676 - accuracy: 0.9186 - val_loss: 1.1887 - val_accuracy: 0.6780\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2253 - accuracy: 0.9326 - val_loss: 1.2615 - val_accuracy: 0.6768\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9464 - val_loss: 1.2994 - val_accuracy: 0.6742\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1537 - accuracy: 0.9576 - val_loss: 1.3546 - val_accuracy: 0.6806\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.1201 - accuracy: 0.9717 - val_loss: 1.3939 - val_accuracy: 0.6745\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.1008 - accuracy: 0.9782 - val_loss: 1.4768 - val_accuracy: 0.6797\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0784 - accuracy: 0.9845 - val_loss: 1.5479 - val_accuracy: 0.6878\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0605 - accuracy: 0.9893 - val_loss: 1.5928 - val_accuracy: 0.6829\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 0.0518 - accuracy: 0.9917 - val_loss: 1.6446 - val_accuracy: 0.6667\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0383 - accuracy: 0.9959 - val_loss: 1.6826 - val_accuracy: 0.6794\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0325 - accuracy: 0.9962 - val_loss: 1.7745 - val_accuracy: 0.6817\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0235 - accuracy: 0.9974 - val_loss: 1.8679 - val_accuracy: 0.6809\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0199 - accuracy: 0.9982 - val_loss: 1.8944 - val_accuracy: 0.6809\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0174 - accuracy: 0.9989 - val_loss: 1.9245 - val_accuracy: 0.6820\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0118 - accuracy: 0.9993 - val_loss: 2.0213 - val_accuracy: 0.6701\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0124 - accuracy: 0.9986 - val_loss: 2.0663 - val_accuracy: 0.6794\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 0.0099 - accuracy: 0.9990 - val_loss: 2.0794 - val_accuracy: 0.6777\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 2.1365 - val_accuracy: 0.6768\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0061 - accuracy: 0.9993 - val_loss: 2.1860 - val_accuracy: 0.6820\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 3s 4ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 2.2293 - val_accuracy: 0.6896\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 2.2590 - val_accuracy: 0.6852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 2.3049 - val_accuracy: 0.6835\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.3049 - accuracy: 0.6835\n",
      "1100.000000-1400.000000     VAL: Loss 2.304901, Accuracy 0.683478\n",
      "creo layer crescente di dimensione  1100\n",
      "creo layer decrescente di dimensione  1700\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 1.3738 - accuracy: 0.5533 - val_loss: 1.1523 - val_accuracy: 0.6197\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 1.0588 - accuracy: 0.6471 - val_loss: 1.1167 - val_accuracy: 0.6342\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 5s 6ms/step - loss: 0.9772 - accuracy: 0.6741 - val_loss: 1.0543 - val_accuracy: 0.6452\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.8839 - accuracy: 0.7037 - val_loss: 1.0281 - val_accuracy: 0.6516\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.8067 - accuracy: 0.7268 - val_loss: 0.9974 - val_accuracy: 0.6554\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.7432 - accuracy: 0.7486 - val_loss: 0.9995 - val_accuracy: 0.6728\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.6651 - accuracy: 0.7730 - val_loss: 1.0007 - val_accuracy: 0.6704\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.6008 - accuracy: 0.8006 - val_loss: 0.9959 - val_accuracy: 0.6745\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 8s 8ms/step - loss: 0.5589 - accuracy: 0.8142 - val_loss: 1.0436 - val_accuracy: 0.6681\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.4988 - accuracy: 0.8371 - val_loss: 1.0405 - val_accuracy: 0.6762\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 7s 7ms/step - loss: 0.4449 - accuracy: 0.8535 - val_loss: 1.0776 - val_accuracy: 0.6713\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.3771 - accuracy: 0.8789 - val_loss: 1.0872 - val_accuracy: 0.6797\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 5s 6ms/step - loss: 0.3274 - accuracy: 0.8954 - val_loss: 1.1516 - val_accuracy: 0.6667\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 6s 6ms/step - loss: 0.2725 - accuracy: 0.9159 - val_loss: 1.2201 - val_accuracy: 0.6713\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.2321 - accuracy: 0.9317 - val_loss: 1.2441 - val_accuracy: 0.6765\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.1871 - accuracy: 0.9466 - val_loss: 1.2793 - val_accuracy: 0.6716\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 5s 5ms/step - loss: 0.1519 - accuracy: 0.9606 - val_loss: 1.3118 - val_accuracy: 0.6762\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.1173 - accuracy: 0.9734 - val_loss: 1.3926 - val_accuracy: 0.6817\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.0949 - accuracy: 0.9808 - val_loss: 1.4573 - val_accuracy: 0.6710\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0754 - accuracy: 0.9853 - val_loss: 1.5194 - val_accuracy: 0.6754\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.0611 - accuracy: 0.9890 - val_loss: 1.6009 - val_accuracy: 0.6745\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0453 - accuracy: 0.9939 - val_loss: 1.6243 - val_accuracy: 0.6696\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0372 - accuracy: 0.9953 - val_loss: 1.7142 - val_accuracy: 0.6713\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0265 - accuracy: 0.9973 - val_loss: 1.7587 - val_accuracy: 0.6780\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0206 - accuracy: 0.9984 - val_loss: 1.8077 - val_accuracy: 0.6797\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0168 - accuracy: 0.9988 - val_loss: 1.8777 - val_accuracy: 0.6786\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9989 - val_loss: 1.9364 - val_accuracy: 0.6852\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0118 - accuracy: 0.9993 - val_loss: 1.9486 - val_accuracy: 0.6803\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0092 - accuracy: 0.9995 - val_loss: 1.9990 - val_accuracy: 0.6820\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 2.0800 - val_accuracy: 0.6797\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9997 - val_loss: 2.1294 - val_accuracy: 0.6768\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 2.1567 - val_accuracy: 0.6817\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0075 - accuracy: 0.9991 - val_loss: 2.1713 - val_accuracy: 0.6855\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 2.3223 - val_accuracy: 0.6867\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 2.3271 - val_accuracy: 0.6757\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0050 - accuracy: 0.9992 - val_loss: 2.3004 - val_accuracy: 0.6864\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 2.3613 - val_accuracy: 0.6841\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0053 - accuracy: 0.9993 - val_loss: 2.3377 - val_accuracy: 0.6783\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.3377 - accuracy: 0.6783\n",
      "1100.000000-1700.000000     VAL: Loss 2.337672, Accuracy 0.678261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc73dff30e9a43cc819b26edd8de7620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creo layer crescente di dimensione  1400\n",
      "creo layer decrescente di dimensione  800\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3801 - accuracy: 0.5512 - val_loss: 1.1805 - val_accuracy: 0.6064\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0699 - accuracy: 0.6485 - val_loss: 1.1021 - val_accuracy: 0.6296\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9727 - accuracy: 0.6786 - val_loss: 1.0590 - val_accuracy: 0.6472\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8992 - accuracy: 0.7023 - val_loss: 1.0225 - val_accuracy: 0.6623\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8313 - accuracy: 0.7216 - val_loss: 0.9926 - val_accuracy: 0.6664\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7729 - accuracy: 0.7402 - val_loss: 0.9814 - val_accuracy: 0.6667\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7059 - accuracy: 0.7612 - val_loss: 0.9789 - val_accuracy: 0.6771\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6482 - accuracy: 0.7805 - val_loss: 0.9792 - val_accuracy: 0.6771\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6099 - accuracy: 0.7946 - val_loss: 0.9948 - val_accuracy: 0.6774\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.8191 - val_loss: 1.0007 - val_accuracy: 0.6759\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4872 - accuracy: 0.8392 - val_loss: 1.0351 - val_accuracy: 0.6820\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4381 - accuracy: 0.8522 - val_loss: 1.0555 - val_accuracy: 0.6806\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3908 - accuracy: 0.8718 - val_loss: 1.0566 - val_accuracy: 0.6832\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3398 - accuracy: 0.8914 - val_loss: 1.0978 - val_accuracy: 0.6780\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.9101 - val_loss: 1.1538 - val_accuracy: 0.6803\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2495 - accuracy: 0.9259 - val_loss: 1.1595 - val_accuracy: 0.6812\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2095 - accuracy: 0.9388 - val_loss: 1.2243 - val_accuracy: 0.6780\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1724 - accuracy: 0.9519 - val_loss: 1.3266 - val_accuracy: 0.6754\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1463 - accuracy: 0.9630 - val_loss: 1.3465 - val_accuracy: 0.6716\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1131 - accuracy: 0.9738 - val_loss: 1.4363 - val_accuracy: 0.6806\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0923 - accuracy: 0.9810 - val_loss: 1.4905 - val_accuracy: 0.6780\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0733 - accuracy: 0.9855 - val_loss: 1.5154 - val_accuracy: 0.6739\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0593 - accuracy: 0.9903 - val_loss: 1.5863 - val_accuracy: 0.6855\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0465 - accuracy: 0.9938 - val_loss: 1.6466 - val_accuracy: 0.6641\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0347 - accuracy: 0.9969 - val_loss: 1.7016 - val_accuracy: 0.6765\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0281 - accuracy: 0.9976 - val_loss: 1.7433 - val_accuracy: 0.6783\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0238 - accuracy: 0.9982 - val_loss: 1.8000 - val_accuracy: 0.6757\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0185 - accuracy: 0.9987 - val_loss: 1.8671 - val_accuracy: 0.6771\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0164 - accuracy: 0.9986 - val_loss: 1.9760 - val_accuracy: 0.6693\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 0.9990 - val_loss: 2.0314 - val_accuracy: 0.6701\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0154 - accuracy: 0.9980 - val_loss: 1.9687 - val_accuracy: 0.6817\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0075 - accuracy: 0.9997 - val_loss: 2.0118 - val_accuracy: 0.6693\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0094 - accuracy: 0.9992 - val_loss: 2.1614 - val_accuracy: 0.6809\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 2.1086 - val_accuracy: 0.6820\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0046 - accuracy: 0.9998 - val_loss: 2.1806 - val_accuracy: 0.6806\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0062 - accuracy: 0.9996 - val_loss: 2.2004 - val_accuracy: 0.6768\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 2.2509 - val_accuracy: 0.6786\n",
      "108/108 [==============================] - 0s 982us/step - loss: 2.2509 - accuracy: 0.6786\n",
      "1400.000000-800.000000     VAL: Loss 2.250876, Accuracy 0.678551\n",
      "creo layer crescente di dimensione  1400\n",
      "creo layer decrescente di dimensione  1100\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3900 - accuracy: 0.5410 - val_loss: 1.1769 - val_accuracy: 0.6099\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0662 - accuracy: 0.6427 - val_loss: 1.1036 - val_accuracy: 0.6345\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9656 - accuracy: 0.6824 - val_loss: 1.0701 - val_accuracy: 0.6484\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8992 - accuracy: 0.7027 - val_loss: 1.0254 - val_accuracy: 0.6643\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8204 - accuracy: 0.7249 - val_loss: 1.0048 - val_accuracy: 0.6580\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7406 - accuracy: 0.7539 - val_loss: 1.0215 - val_accuracy: 0.6713\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6905 - accuracy: 0.7684 - val_loss: 1.0140 - val_accuracy: 0.6765\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6362 - accuracy: 0.7881 - val_loss: 1.0019 - val_accuracy: 0.6754\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5547 - accuracy: 0.8151 - val_loss: 1.0289 - val_accuracy: 0.6742\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4997 - accuracy: 0.8346 - val_loss: 1.0591 - val_accuracy: 0.6751\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4499 - accuracy: 0.8540 - val_loss: 1.0877 - val_accuracy: 0.6777\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4019 - accuracy: 0.8699 - val_loss: 1.0948 - val_accuracy: 0.6783\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3368 - accuracy: 0.8908 - val_loss: 1.1227 - val_accuracy: 0.6783\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.9102 - val_loss: 1.1659 - val_accuracy: 0.6786\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2411 - accuracy: 0.9275 - val_loss: 1.2133 - val_accuracy: 0.6765\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9430 - val_loss: 1.3137 - val_accuracy: 0.6730\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1654 - accuracy: 0.9548 - val_loss: 1.3255 - val_accuracy: 0.6730\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1297 - accuracy: 0.9679 - val_loss: 1.3779 - val_accuracy: 0.6786\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1021 - accuracy: 0.9772 - val_loss: 1.4424 - val_accuracy: 0.6788\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0777 - accuracy: 0.9860 - val_loss: 1.5140 - val_accuracy: 0.6812\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0619 - accuracy: 0.9897 - val_loss: 1.5509 - val_accuracy: 0.6771\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0512 - accuracy: 0.9921 - val_loss: 1.6548 - val_accuracy: 0.6693\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0392 - accuracy: 0.9949 - val_loss: 1.7179 - val_accuracy: 0.6658\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0345 - accuracy: 0.9952 - val_loss: 1.6996 - val_accuracy: 0.6757\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0244 - accuracy: 0.9980 - val_loss: 1.7577 - val_accuracy: 0.6820\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0170 - accuracy: 0.9989 - val_loss: 1.8694 - val_accuracy: 0.6786\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0159 - accuracy: 0.9990 - val_loss: 1.9533 - val_accuracy: 0.6814\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0138 - accuracy: 0.9988 - val_loss: 2.0033 - val_accuracy: 0.6829\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9995 - val_loss: 2.0280 - val_accuracy: 0.6870\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0071 - accuracy: 0.9996 - val_loss: 2.0766 - val_accuracy: 0.6806\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0067 - accuracy: 0.9998 - val_loss: 2.1523 - val_accuracy: 0.6846\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0060 - accuracy: 0.9996 - val_loss: 2.1854 - val_accuracy: 0.6780\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0052 - accuracy: 0.9996 - val_loss: 2.1879 - val_accuracy: 0.6864\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 2.1918 - val_accuracy: 0.6864\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 2.2463 - val_accuracy: 0.6849\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 2.2802 - val_accuracy: 0.6809\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0049 - accuracy: 0.9993 - val_loss: 2.3584 - val_accuracy: 0.6809\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 2.3565 - val_accuracy: 0.6826\n",
      "108/108 [==============================] - 0s 830us/step - loss: 2.3565 - accuracy: 0.6826\n",
      "1400.000000-1100.000000     VAL: Loss 2.356480, Accuracy 0.682609\n",
      "creo layer crescente di dimensione  1400\n",
      "creo layer decrescente di dimensione  1400\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3623 - accuracy: 0.5548 - val_loss: 1.1788 - val_accuracy: 0.6136\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0505 - accuracy: 0.6508 - val_loss: 1.0807 - val_accuracy: 0.6406\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9462 - accuracy: 0.6861 - val_loss: 1.0513 - val_accuracy: 0.6490\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8697 - accuracy: 0.7076 - val_loss: 1.0292 - val_accuracy: 0.6571\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8026 - accuracy: 0.7285 - val_loss: 1.0149 - val_accuracy: 0.6652\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7384 - accuracy: 0.7507 - val_loss: 0.9851 - val_accuracy: 0.6730\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6550 - accuracy: 0.7791 - val_loss: 0.9884 - val_accuracy: 0.6780\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6108 - accuracy: 0.7945 - val_loss: 1.0411 - val_accuracy: 0.6600\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.8203 - val_loss: 1.0258 - val_accuracy: 0.6771\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4834 - accuracy: 0.8365 - val_loss: 1.0630 - val_accuracy: 0.6739\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4148 - accuracy: 0.8629 - val_loss: 1.0862 - val_accuracy: 0.6783\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3554 - accuracy: 0.8845 - val_loss: 1.0973 - val_accuracy: 0.6780\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3076 - accuracy: 0.9017 - val_loss: 1.1396 - val_accuracy: 0.6765\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2536 - accuracy: 0.9227 - val_loss: 1.2130 - val_accuracy: 0.6788\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9390 - val_loss: 1.2514 - val_accuracy: 0.6736\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1690 - accuracy: 0.9559 - val_loss: 1.2976 - val_accuracy: 0.6736\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 3s 3ms/step - loss: 0.1395 - accuracy: 0.9642 - val_loss: 1.3911 - val_accuracy: 0.6832\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9793 - val_loss: 1.4642 - val_accuracy: 0.6803\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0842 - accuracy: 0.9836 - val_loss: 1.4866 - val_accuracy: 0.6823\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0664 - accuracy: 0.9874 - val_loss: 1.5754 - val_accuracy: 0.6759\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0517 - accuracy: 0.9917 - val_loss: 1.6403 - val_accuracy: 0.6814\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0404 - accuracy: 0.9948 - val_loss: 1.6816 - val_accuracy: 0.6838\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0334 - accuracy: 0.9956 - val_loss: 1.7724 - val_accuracy: 0.6878\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9970 - val_loss: 1.8101 - val_accuracy: 0.6786\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0194 - accuracy: 0.9980 - val_loss: 1.8740 - val_accuracy: 0.6832\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0150 - accuracy: 0.9986 - val_loss: 1.8939 - val_accuracy: 0.6899\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0106 - accuracy: 0.9995 - val_loss: 1.9707 - val_accuracy: 0.6861\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9994 - val_loss: 2.0331 - val_accuracy: 0.6867\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9996 - val_loss: 2.0589 - val_accuracy: 0.6878\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0080 - accuracy: 0.9995 - val_loss: 2.1358 - val_accuracy: 0.6745\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0056 - accuracy: 0.9997 - val_loss: 2.1622 - val_accuracy: 0.6867\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 2.2242 - val_accuracy: 0.6797\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0064 - accuracy: 0.9993 - val_loss: 2.3383 - val_accuracy: 0.6843\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0065 - accuracy: 0.9993 - val_loss: 2.2720 - val_accuracy: 0.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 2.3002 - val_accuracy: 0.6872\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 2.3805 - val_accuracy: 0.6875\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.3805 - accuracy: 0.6875\n",
      "1400.000000-1400.000000     VAL: Loss 2.380479, Accuracy 0.687536\n",
      "creo layer crescente di dimensione  1400\n",
      "creo layer decrescente di dimensione  1700\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3648 - accuracy: 0.5527 - val_loss: 1.1669 - val_accuracy: 0.6136\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0542 - accuracy: 0.6523 - val_loss: 1.1125 - val_accuracy: 0.6316\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9503 - accuracy: 0.6844 - val_loss: 1.0599 - val_accuracy: 0.6452\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8821 - accuracy: 0.7005 - val_loss: 1.0247 - val_accuracy: 0.6614\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7948 - accuracy: 0.7306 - val_loss: 1.0297 - val_accuracy: 0.6664\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7214 - accuracy: 0.7551 - val_loss: 1.0198 - val_accuracy: 0.6777\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6394 - accuracy: 0.7833 - val_loss: 1.0190 - val_accuracy: 0.6762\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5775 - accuracy: 0.8072 - val_loss: 1.0291 - val_accuracy: 0.6765\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5057 - accuracy: 0.8313 - val_loss: 1.0411 - val_accuracy: 0.6757\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4458 - accuracy: 0.8517 - val_loss: 1.0657 - val_accuracy: 0.6725\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3869 - accuracy: 0.8703 - val_loss: 1.1084 - val_accuracy: 0.6867\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3346 - accuracy: 0.8946 - val_loss: 1.1509 - val_accuracy: 0.6823\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.9125 - val_loss: 1.1969 - val_accuracy: 0.6823\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2152 - accuracy: 0.9355 - val_loss: 1.2607 - val_accuracy: 0.6800\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9507 - val_loss: 1.3992 - val_accuracy: 0.6632\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1408 - accuracy: 0.9648 - val_loss: 1.4373 - val_accuracy: 0.6774\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1067 - accuracy: 0.9755 - val_loss: 1.4882 - val_accuracy: 0.6817\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0806 - accuracy: 0.9849 - val_loss: 1.5714 - val_accuracy: 0.6736\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0614 - accuracy: 0.9893 - val_loss: 1.6246 - val_accuracy: 0.6817\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0501 - accuracy: 0.9912 - val_loss: 1.7426 - val_accuracy: 0.6701\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0382 - accuracy: 0.9952 - val_loss: 1.7535 - val_accuracy: 0.6806\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0260 - accuracy: 0.9979 - val_loss: 1.8090 - val_accuracy: 0.6867\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0219 - accuracy: 0.9978 - val_loss: 1.9212 - val_accuracy: 0.6820\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0186 - accuracy: 0.9983 - val_loss: 1.9699 - val_accuracy: 0.6878\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0143 - accuracy: 0.9987 - val_loss: 2.0078 - val_accuracy: 0.6826\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0116 - accuracy: 0.9992 - val_loss: 2.1563 - val_accuracy: 0.6823\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0116 - accuracy: 0.9986 - val_loss: 2.0968 - val_accuracy: 0.6846\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0092 - accuracy: 0.9988 - val_loss: 2.1687 - val_accuracy: 0.6901\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9990 - val_loss: 2.2150 - val_accuracy: 0.6835\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0040 - accuracy: 0.9999 - val_loss: 2.2957 - val_accuracy: 0.6768\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0080 - accuracy: 0.9992 - val_loss: 2.3194 - val_accuracy: 0.6846\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 2.4138 - val_accuracy: 0.6771\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0081 - accuracy: 0.9992 - val_loss: 2.3904 - val_accuracy: 0.6849\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 2.4674 - val_accuracy: 0.6872\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 2.5142 - val_accuracy: 0.6829\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6418 - val_accuracy: 0.6664\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 2.6003 - val_accuracy: 0.6809\n",
      "108/108 [==============================] - 0s 956us/step - loss: 2.6003 - accuracy: 0.6809\n",
      "1400.000000-1700.000000     VAL: Loss 2.600283, Accuracy 0.680870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cfb37eaa12453398981e6d9e5a62b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creo layer crescente di dimensione  1700\n",
      "creo layer decrescente di dimensione  800\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3674 - accuracy: 0.5496 - val_loss: 1.1588 - val_accuracy: 0.6214\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0749 - accuracy: 0.6410 - val_loss: 1.0874 - val_accuracy: 0.6304\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9627 - accuracy: 0.6800 - val_loss: 1.0399 - val_accuracy: 0.6513\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8943 - accuracy: 0.6999 - val_loss: 1.0043 - val_accuracy: 0.6649\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8201 - accuracy: 0.7222 - val_loss: 1.0117 - val_accuracy: 0.6658\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7611 - accuracy: 0.7452 - val_loss: 0.9997 - val_accuracy: 0.6745\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6857 - accuracy: 0.7676 - val_loss: 0.9870 - val_accuracy: 0.6733\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6341 - accuracy: 0.7852 - val_loss: 0.9806 - val_accuracy: 0.6794\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5732 - accuracy: 0.8039 - val_loss: 0.9761 - val_accuracy: 0.6820\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5158 - accuracy: 0.8285 - val_loss: 1.0022 - val_accuracy: 0.6794\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.8523 - val_loss: 1.0192 - val_accuracy: 0.6817\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3935 - accuracy: 0.8723 - val_loss: 1.0696 - val_accuracy: 0.6814\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3529 - accuracy: 0.8834 - val_loss: 1.0929 - val_accuracy: 0.6814\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2982 - accuracy: 0.9048 - val_loss: 1.1562 - val_accuracy: 0.6864\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2542 - accuracy: 0.9209 - val_loss: 1.1725 - val_accuracy: 0.6884\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2055 - accuracy: 0.9409 - val_loss: 1.2402 - val_accuracy: 0.6829\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1695 - accuracy: 0.9537 - val_loss: 1.2852 - val_accuracy: 0.6777\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1387 - accuracy: 0.9637 - val_loss: 1.3262 - val_accuracy: 0.6786\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1118 - accuracy: 0.9743 - val_loss: 1.3809 - val_accuracy: 0.6835\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0887 - accuracy: 0.9804 - val_loss: 1.4579 - val_accuracy: 0.6809\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0631 - accuracy: 0.9899 - val_loss: 1.5699 - val_accuracy: 0.6849\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0493 - accuracy: 0.9926 - val_loss: 1.5686 - val_accuracy: 0.6826\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0418 - accuracy: 0.9938 - val_loss: 1.6234 - val_accuracy: 0.6788\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0292 - accuracy: 0.9973 - val_loss: 1.7192 - val_accuracy: 0.6884\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0278 - accuracy: 0.9962 - val_loss: 1.8054 - val_accuracy: 0.6788\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9976 - val_loss: 1.8481 - val_accuracy: 0.6838\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0168 - accuracy: 0.9990 - val_loss: 1.8901 - val_accuracy: 0.6748\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 0.9983 - val_loss: 1.9447 - val_accuracy: 0.6870\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9996 - val_loss: 1.9759 - val_accuracy: 0.6814\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0082 - accuracy: 0.9997 - val_loss: 2.0351 - val_accuracy: 0.6907\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0091 - accuracy: 0.9992 - val_loss: 2.0558 - val_accuracy: 0.6846\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9994 - val_loss: 2.1093 - val_accuracy: 0.6826\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0072 - accuracy: 0.9993 - val_loss: 2.1346 - val_accuracy: 0.6838\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0044 - accuracy: 0.9997 - val_loss: 2.2209 - val_accuracy: 0.6838\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 2.1837 - val_accuracy: 0.6867\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 2.2577 - val_accuracy: 0.6890\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 2.2614 - val_accuracy: 0.6933\n",
      "Epoch 38/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 2.2971 - val_accuracy: 0.6875\n",
      "Epoch 39/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 2.3978 - val_accuracy: 0.6829\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 2.3978 - accuracy: 0.6829\n",
      "1700.000000-800.000000     VAL: Loss 2.397828, Accuracy 0.682899\n",
      "creo layer crescente di dimensione  1700\n",
      "creo layer decrescente di dimensione  1100\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3780 - accuracy: 0.5464 - val_loss: 1.1632 - val_accuracy: 0.6136\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 5s 6ms/step - loss: 1.0587 - accuracy: 0.6494 - val_loss: 1.1016 - val_accuracy: 0.6275\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 4s 4ms/step - loss: 0.9517 - accuracy: 0.6819 - val_loss: 1.0584 - val_accuracy: 0.6414\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8757 - accuracy: 0.7081 - val_loss: 1.0312 - val_accuracy: 0.6548\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8042 - accuracy: 0.7287 - val_loss: 1.0137 - val_accuracy: 0.6638\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7299 - accuracy: 0.7531 - val_loss: 1.0000 - val_accuracy: 0.6704\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6694 - accuracy: 0.7740 - val_loss: 1.0204 - val_accuracy: 0.6684\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5936 - accuracy: 0.7995 - val_loss: 1.0270 - val_accuracy: 0.6684\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5488 - accuracy: 0.8156 - val_loss: 1.0151 - val_accuracy: 0.6791\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4701 - accuracy: 0.8452 - val_loss: 1.0438 - val_accuracy: 0.6812\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8631 - val_loss: 1.0831 - val_accuracy: 0.6751\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8881 - val_loss: 1.0965 - val_accuracy: 0.6875\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3020 - accuracy: 0.9050 - val_loss: 1.1634 - val_accuracy: 0.6725\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2447 - accuracy: 0.9269 - val_loss: 1.2263 - val_accuracy: 0.6797\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9429 - val_loss: 1.2488 - val_accuracy: 0.6881\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1665 - accuracy: 0.9541 - val_loss: 1.3238 - val_accuracy: 0.6783\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1296 - accuracy: 0.9683 - val_loss: 1.3962 - val_accuracy: 0.6780\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1031 - accuracy: 0.9747 - val_loss: 1.4421 - val_accuracy: 0.6791\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0807 - accuracy: 0.9829 - val_loss: 1.4967 - val_accuracy: 0.6843\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0598 - accuracy: 0.9904 - val_loss: 1.6248 - val_accuracy: 0.6852\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0487 - accuracy: 0.9918 - val_loss: 1.6397 - val_accuracy: 0.6820\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0360 - accuracy: 0.9953 - val_loss: 1.7418 - val_accuracy: 0.6855\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0249 - accuracy: 0.9977 - val_loss: 1.7726 - val_accuracy: 0.6716\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0232 - accuracy: 0.9977 - val_loss: 1.8510 - val_accuracy: 0.6864\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0173 - accuracy: 0.9985 - val_loss: 1.8757 - val_accuracy: 0.6817\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0171 - accuracy: 0.9980 - val_loss: 1.9511 - val_accuracy: 0.6858\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9987 - val_loss: 1.9738 - val_accuracy: 0.6875\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0087 - accuracy: 0.9991 - val_loss: 2.0412 - val_accuracy: 0.6875\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0070 - accuracy: 0.9996 - val_loss: 2.0990 - val_accuracy: 0.6826\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0072 - accuracy: 0.9995 - val_loss: 2.1228 - val_accuracy: 0.6896\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 2.3055 - val_accuracy: 0.6858\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0086 - accuracy: 0.9990 - val_loss: 2.1707 - val_accuracy: 0.6933\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 2.5036 - val_accuracy: 0.6788\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 2.3083 - val_accuracy: 0.6919\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 2.3047 - val_accuracy: 0.6838\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0062 - accuracy: 0.9993 - val_loss: 2.3248 - val_accuracy: 0.6928\n",
      "108/108 [==============================] - 0s 907us/step - loss: 2.3248 - accuracy: 0.6928\n",
      "1700.000000-1100.000000     VAL: Loss 2.324766, Accuracy 0.692754\n",
      "creo layer crescente di dimensione  1700\n",
      "creo layer decrescente di dimensione  1400\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3640 - accuracy: 0.5503 - val_loss: 1.1858 - val_accuracy: 0.6067\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0483 - accuracy: 0.6508 - val_loss: 1.0715 - val_accuracy: 0.6383\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9620 - accuracy: 0.6756 - val_loss: 1.0803 - val_accuracy: 0.6386\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8742 - accuracy: 0.7040 - val_loss: 1.0301 - val_accuracy: 0.6499\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7992 - accuracy: 0.7282 - val_loss: 1.0093 - val_accuracy: 0.6629\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7222 - accuracy: 0.7549 - val_loss: 1.0155 - val_accuracy: 0.6664\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6582 - accuracy: 0.7798 - val_loss: 1.0024 - val_accuracy: 0.6710\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5792 - accuracy: 0.8036 - val_loss: 1.0299 - val_accuracy: 0.6774\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5166 - accuracy: 0.8255 - val_loss: 1.0517 - val_accuracy: 0.6736\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4518 - accuracy: 0.8519 - val_loss: 1.0910 - val_accuracy: 0.6699\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3897 - accuracy: 0.8723 - val_loss: 1.1102 - val_accuracy: 0.6771\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3231 - accuracy: 0.8957 - val_loss: 1.1707 - val_accuracy: 0.6803\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2682 - accuracy: 0.9162 - val_loss: 1.2297 - val_accuracy: 0.6861\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2206 - accuracy: 0.9342 - val_loss: 1.2801 - val_accuracy: 0.6878\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9481 - val_loss: 1.3473 - val_accuracy: 0.6823\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1410 - accuracy: 0.9618 - val_loss: 1.4556 - val_accuracy: 0.6835\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1101 - accuracy: 0.9729 - val_loss: 1.4719 - val_accuracy: 0.6794\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0807 - accuracy: 0.9841 - val_loss: 1.5862 - val_accuracy: 0.6806\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0594 - accuracy: 0.9906 - val_loss: 1.6969 - val_accuracy: 0.6800\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0444 - accuracy: 0.9936 - val_loss: 1.6957 - val_accuracy: 0.6725\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0362 - accuracy: 0.9950 - val_loss: 1.7448 - val_accuracy: 0.6768\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 0.9963 - val_loss: 1.8435 - val_accuracy: 0.6777\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9984 - val_loss: 1.9597 - val_accuracy: 0.6838\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0174 - accuracy: 0.9984 - val_loss: 1.9660 - val_accuracy: 0.6832\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0106 - accuracy: 0.9993 - val_loss: 2.0829 - val_accuracy: 0.6812\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0127 - accuracy: 0.9984 - val_loss: 2.1431 - val_accuracy: 0.6843\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 2.1619 - val_accuracy: 0.6759\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0090 - accuracy: 0.9989 - val_loss: 2.2021 - val_accuracy: 0.6849\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0072 - accuracy: 0.9997 - val_loss: 2.2836 - val_accuracy: 0.6783\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0068 - accuracy: 0.9991 - val_loss: 2.2648 - val_accuracy: 0.6846\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0040 - accuracy: 0.9997 - val_loss: 2.3413 - val_accuracy: 0.6780\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 2.4195 - val_accuracy: 0.6826\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 2.4379 - val_accuracy: 0.6890\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0038 - accuracy: 0.9995 - val_loss: 2.4526 - val_accuracy: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 2.5808 - val_accuracy: 0.6809\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 2.5506 - val_accuracy: 0.6907\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 2.5067 - val_accuracy: 0.6849\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.5067 - accuracy: 0.6849\n",
      "1700.000000-1400.000000     VAL: Loss 2.506659, Accuracy 0.684928\n",
      "creo layer crescente di dimensione  1700\n",
      "creo layer decrescente di dimensione  1700\n",
      "Epoch 1/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.3720 - accuracy: 0.5506 - val_loss: 1.1566 - val_accuracy: 0.6214\n",
      "Epoch 2/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 1.0485 - accuracy: 0.6547 - val_loss: 1.0868 - val_accuracy: 0.6391\n",
      "Epoch 3/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.9420 - accuracy: 0.6869 - val_loss: 1.0518 - val_accuracy: 0.6519\n",
      "Epoch 4/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.8784 - accuracy: 0.7076 - val_loss: 1.0332 - val_accuracy: 0.6562\n",
      "Epoch 5/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7786 - accuracy: 0.7370 - val_loss: 1.0201 - val_accuracy: 0.6699\n",
      "Epoch 6/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.7131 - accuracy: 0.7589 - val_loss: 1.0237 - val_accuracy: 0.6655\n",
      "Epoch 7/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.6300 - accuracy: 0.7913 - val_loss: 0.9987 - val_accuracy: 0.6745\n",
      "Epoch 8/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.5595 - accuracy: 0.8123 - val_loss: 1.0215 - val_accuracy: 0.6762\n",
      "Epoch 9/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4994 - accuracy: 0.8312 - val_loss: 1.0503 - val_accuracy: 0.6646\n",
      "Epoch 10/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8578 - val_loss: 1.1167 - val_accuracy: 0.6614\n",
      "Epoch 11/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3616 - accuracy: 0.8801 - val_loss: 1.1531 - val_accuracy: 0.6719\n",
      "Epoch 12/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.3037 - accuracy: 0.9028 - val_loss: 1.1754 - val_accuracy: 0.6641\n",
      "Epoch 13/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.2452 - accuracy: 0.9235 - val_loss: 1.2734 - val_accuracy: 0.6664\n",
      "Epoch 14/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9434 - val_loss: 1.2704 - val_accuracy: 0.6797\n",
      "Epoch 15/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1515 - accuracy: 0.9592 - val_loss: 1.3504 - val_accuracy: 0.6716\n",
      "Epoch 16/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.1162 - accuracy: 0.9709 - val_loss: 1.4152 - val_accuracy: 0.6565\n",
      "Epoch 17/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0904 - accuracy: 0.9807 - val_loss: 1.5159 - val_accuracy: 0.6783\n",
      "Epoch 18/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0651 - accuracy: 0.9887 - val_loss: 1.5826 - val_accuracy: 0.6728\n",
      "Epoch 19/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0464 - accuracy: 0.9927 - val_loss: 1.6910 - val_accuracy: 0.6794\n",
      "Epoch 20/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0352 - accuracy: 0.9951 - val_loss: 1.7546 - val_accuracy: 0.6783\n",
      "Epoch 21/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0263 - accuracy: 0.9970 - val_loss: 1.8065 - val_accuracy: 0.6728\n",
      "Epoch 22/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0250 - accuracy: 0.9971 - val_loss: 1.8796 - val_accuracy: 0.6771\n",
      "Epoch 23/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9985 - val_loss: 1.9383 - val_accuracy: 0.6768\n",
      "Epoch 24/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0156 - accuracy: 0.9981 - val_loss: 2.0254 - val_accuracy: 0.6806\n",
      "Epoch 25/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 2.0391 - val_accuracy: 0.6742\n",
      "Epoch 26/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0073 - accuracy: 0.9997 - val_loss: 2.0922 - val_accuracy: 0.6852\n",
      "Epoch 27/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0091 - accuracy: 0.9994 - val_loss: 2.1423 - val_accuracy: 0.6783\n",
      "Epoch 28/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 2.2759 - val_accuracy: 0.6670\n",
      "Epoch 29/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0078 - accuracy: 0.9991 - val_loss: 2.2938 - val_accuracy: 0.6777\n",
      "Epoch 30/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0066 - accuracy: 0.9990 - val_loss: 2.2818 - val_accuracy: 0.6858\n",
      "Epoch 31/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 2.4071 - val_accuracy: 0.6777\n",
      "Epoch 32/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 2.3396 - val_accuracy: 0.6806\n",
      "Epoch 33/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 2.3649 - val_accuracy: 0.6788\n",
      "Epoch 34/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 2.4307 - val_accuracy: 0.6742\n",
      "Epoch 35/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9979 - val_loss: 2.4242 - val_accuracy: 0.6762\n",
      "Epoch 36/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 2.4937 - val_accuracy: 0.6728\n",
      "Epoch 37/1000\n",
      "971/971 [==============================] - 2s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 2.5251 - val_accuracy: 0.6797\n",
      "108/108 [==============================] - 0s 898us/step - loss: 2.5251 - accuracy: 0.6797\n",
      "1700.000000-1700.000000     VAL: Loss 2.525063, Accuracy 0.679710\n"
     ]
    }
   ],
   "source": [
    "layer1 = range(600, 1600, 200)\n",
    "layer2 = range(300, 1300 +1, 200)\n",
    "\n",
    "result = {}\n",
    "\n",
    "for a in tqdm(layer1):\n",
    "    for b in tqdm(layer2):\n",
    "        model = costruisci(n_layer_crescenti=1, \n",
    "                   initialHiddenLayer=a,\n",
    "                      n_layer_decrescenti = 1,\n",
    "                      finalHiddenLayer=b)\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', patience=30)\n",
    "        mc = ModelCheckpoint('best_model_NOREG.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "        history1 = model.fit(X_res, y_res, epochs=1000, \n",
    "                              #batch_size= 200,\n",
    "                              #class_weight=class_weight,\n",
    "                              validation_data=(X_val, y_val), callbacks=[es,mc]\n",
    "                             ).history\n",
    "        \n",
    "        test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "        \n",
    "        result[str(a)+\"-\"+str(b)] = str(test_acc) + str( test_loss)\n",
    "        \n",
    "        print('%f-%f     VAL: Loss %f, Accuracy %f' % (a, b, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'800-800': '0.6762318611145022.2404749393463135',\n",
       " '800-1100': '0.6762318611145022.3196237087249756',\n",
       " '1100-1700': '0.6782608628273012.337672472000122',\n",
       " '1400-800': '0.67855072021484382.25087571144104',\n",
       " '1100-800': '0.67942029237747192.2469706535339355',\n",
       " '800-1700': '0.67942029237747192.365679979324341',\n",
       " '1700-1700': '0.67971014976501462.5250625610351562',\n",
       " '1400-1700': '0.68086957931518552.60028338432312',\n",
       " '800-1400': '0.6814492940902712.2880008220672607',\n",
       " '1100-1100': '0.68202900886535642.4322619438171387',\n",
       " '1400-1100': '0.68260872364044192.356480121612549',\n",
       " '1700-800': '0.68289852142333982.3978283405303955',\n",
       " '1100-1400': '0.68347823619842532.304901361465454',\n",
       " '1700-1400': '0.68492752313613892.5066585540771484',\n",
       " '1400-1400': '0.68753623962402342.380479335784912',\n",
       " '1700-1100': '0.69275361299514772.324765920639038'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(result.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'800-1200': '0.6924637556076052.134887456893921',\n",
    " '1200-1400': '0.6924637556076052.2831122875213623',\n",
    " '1300-1100': '0.69333332777023322.321800947189331',\n",
    " '1200-1700': '0.6944927573204042.3965771198272705',\n",
    " '1200-1300': '0.69855070114135742.325577735900879',\n",
    " '1100-1500': '0.6999999880790712.2375428676605225'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1610 - accuracy: 0.6846\n",
      "VAL: Loss 1.161048, Accuracy 0.684638\n",
      "462/462 [==============================] - 0s 1ms/step - loss: 1.1402 - accuracy: 0.6746\n",
      "TEST: Loss 1.140161, Accuracy 0.674559\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "\n",
    "print('VAL: Loss %f, Accuracy %f' % (test_loss, test_acc))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('TEST: Loss %f, Accuracy %f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per runnare il modello finchè non è trovato un modello migliore\n",
    "accuracyDaSuperare = 0.0\n",
    "\n",
    "while True:\n",
    "    model = costruisciTop(n_layer_crescenti=4, \n",
    "                   initialHiddenLayer=80,\n",
    "                      n_layer_decrescenti = 1,\n",
    "                      finalHiddenLayer=300)\n",
    "\n",
    "\n",
    "    history1 = model.fit(X_res, y_res, epochs=1000, \n",
    "                          #batch_size= 128,\n",
    "                          #class_weight=class_weight,\n",
    "                          validation_data=(X_val, y_val), callbacks=[es,mc]\n",
    "                         ).history\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "    \n",
    "    if(test_acc > accuracyDaSuperare):\n",
    "        break\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modello_top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_bool = np.argmax(y_pred, axis=1) #si deve usare argmax o predict_classes\n",
    "y_test_bool = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_bool, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classi predette e istanze per classe\n",
    "\n",
    "y_pred = model1.predict_classes(X_test)\n",
    "\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzione per svuotare la VRAM nel caso non vengo fatto in automatico\n",
    "K.clear_session()\n",
    "\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
